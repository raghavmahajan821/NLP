{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN426Jh/wj+BG8igb8d2rpn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raghavmahajan821/NLP/blob/main/Exploring_%26_Processing_Text_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Processing Text Data**\n",
        "\n",
        "We are going to discuss the following recipes under text preprocessing\n",
        "and exploratory data analysis.\n",
        "\n",
        "Recipe 1. Lowercasing\n",
        "\n",
        "Recipe 2. Punctuation removal\n",
        "\n",
        "Recipe 3. Stop words removal\n",
        "\n",
        "Recipe 4. Text standardization\n",
        "\n",
        "Recipe 5. Spelling correction\n",
        "\n",
        "Recipe 6. Tokenization\n",
        "\n",
        "Recipe 7. Stemming\n",
        "\n",
        "Recipe 8. Lemmatization\n",
        "\n",
        "Recipe 9. Exploratory data analysis\n",
        "\n",
        "Recipe 10. End-to-end processing pipeline"
      ],
      "metadata": {
        "id": "z1lce5Lfaf9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ceS5vrDYasNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8999d843-b7f9-418f-e5f7-e92b9edaff13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text in the form of dataframe"
      ],
      "metadata": {
        "id": "ex6au6hV4GeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Text = [\n",
        "    'This @is %introduction %to &@NLP',\n",
        "    'It is likely to be #useful to people',\n",
        "    'Machine &*learning is the new electricity',\n",
        "    'There ^ would be less hype around AI and more action going forward',\n",
        "    'python is the best tool!',\n",
        "    'R is a good language',\n",
        "    'I like this #&book',\n",
        "    'I want more books like this'\n",
        "]                                           #write in this way only spaces matter\n",
        "\n",
        "\n",
        "# crreating dataframe from list\n",
        "df=pd.DataFrame({'tweet':Text})\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfPSIDY5a5gY",
        "outputId": "3e435094-33ae-49a0-b240-5cfd0b447690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet\n",
            "0                   This @is %introduction %to &@NLP\n",
            "1               It is likely to be #useful to people\n",
            "2          Machine &*learning is the new electricity\n",
            "3  There ^ would be less hype around AI and more ...\n",
            "4                           python is the best tool!\n",
            "5                               R is a good language\n",
            "6                                 I like this #&book\n",
            "7                        I want more books like this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Normal Text"
      ],
      "metadata": {
        "id": "6HUQdWGS4PM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural !@ language processing (NLP) is a #!field \" + \\\n",
        "       \"of computer science, artificial intelligence@ \" + \\\n",
        "       \"and computational linguistics concerned with \" + \\\n",
        "       \"the interactions ^ between computers and human \" + \\\n",
        "       \"(natural) languages. In particular, \" + \\\n",
        "       \"concerned with & programming computers to \" + \\\n",
        "       \"fruitfully %% process large natural language \" + \\\n",
        "       \"corpora. Challenges in natural language. \" + \\\n",
        "       \"Processing frequently ##involve natural \" + \\\n",
        "       \"language understanding, natural language\" + \\\n",
        "       \"generation frequently from formal, machine\" + \\\n",
        "       \"-readable logical forms), connecting language \" + \\\n",
        "       \"and machine perception, managing human-\" + \\\n",
        "       \"computer dialog systems, or some combination \" + \\\n",
        "       \"thereof.\"\n",
        "# text = \"This is a sample sentence for tokenization.\"\n",
        "print(text)"
      ],
      "metadata": {
        "id": "3x1vwuO8r7ek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481ce5a5-33e6-4db3-d86a-f5bf80c9bdf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural !@ language processing (NLP) is a #!field of computer science, artificial intelligence@ and computational linguistics concerned with the interactions ^ between computers and human (natural) languages. In particular, concerned with & programming computers to fruitfully %% process large natural language corpora. Challenges in natural language. Processing frequently ##involve natural language understanding, natural languagegeneration frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 1- LowerCasing"
      ],
      "metadata": {
        "id": "NoyYNgEz5l1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowercasing strings\n",
        "text=text.lower()\n",
        "print(text)\n",
        "print('\\n\\n')\n",
        "\n",
        "# Lowercasing dataframes\n",
        "df['tweet']=df['tweet'].apply(lambda x:\" \".join (x.lower() for x in x.split()))\n",
        "print(df['tweet'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlYOpewdsFU_",
        "outputId": "2d1ce538-f601-43c5-d124-37a1f0973b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natural !@ language processing (nlp) is a #!field of computer science, artificial intelligence@ and computational linguistics concerned with the interactions ^ between computers and human (natural) languages. in particular, concerned with & programming computers to fruitfully %% process large natural language corpora. challenges in natural language. processing frequently ##involve natural language understanding, natural languagegeneration frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\n",
            "\n",
            "\n",
            "\n",
            "0                     this @is %introduction %to &@nlp\n",
            "1                 it is likely to be #useful to people\n",
            "2            machine &*learning is the new electricity\n",
            "3    there ^ would be less hype around ai and more ...\n",
            "4                             python is the best tool!\n",
            "5                                 r is a good language\n",
            "6                                   i like this #&book\n",
            "7                          i want more books like this\n",
            "Name: tweet, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 2-Punctuation Removal"
      ],
      "metadata": {
        "id": "cXf_My3P57qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text=re.sub(r'[^\\w\\s]',' ',text)\n",
        "print(text)\n",
        "print('\\n\\n\\n')\n",
        "\n",
        "# Punctuation removal in dataframe\n",
        "df['tweet'] = df['tweet'].apply(lambda x: \" \".join(re.sub(r'[^\\w\\s]', '', x).split()))\n",
        "print(df['tweet'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iDbOz_y_Wsr",
        "outputId": "0bcd7815-6679-4649-f55c-b7ad33195571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natural    language processing  nlp  is a   field of computer science  artificial intelligence  and computational linguistics concerned with the interactions   between computers and human  natural  languages  in particular  concerned with   programming computers to fruitfully    process large natural language corpora  challenges in natural language  processing frequently   involve natural language understanding  natural languagegeneration frequently from formal  machine readable logical forms   connecting language and machine perception  managing human computer dialog systems  or some combination thereof \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0                          this is introduction to nlp\n",
            "1                  it is likely to be useful to people\n",
            "2              machine learning is the new electricity\n",
            "3    there would be less hype around ai and more ac...\n",
            "4                              python is the best tool\n",
            "5                                 r is a good language\n",
            "6                                     i like this book\n",
            "7                          i want more books like this\n",
            "Name: tweet, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 3- Stop words removal"
      ],
      "metadata": {
        "id": "l86DtcI9-x-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop=stopwords.words('english')\n",
        "df['tweet']=df['tweet'].apply(lambda x:\" \". join (x for x in x.split() if x not in stop))\n",
        "print(df['tweet'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4BS8oLuGRNR",
        "outputId": "e87af9df-4328-4964-e685-ea5544926992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                  introduction nlp\n",
            "1                              likely useful people\n",
            "2                  machine learning new electricity\n",
            "3    would less hype around ai action going forward\n",
            "4                                  python best tool\n",
            "5                                   r good language\n",
            "6                                         like book\n",
            "7                                   want books like\n",
            "Name: tweet, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 5-Spelling Correction"
      ],
      "metadata": {
        "id": "oJlJ1NnSQDYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "# Example text with spelling errors\n",
        "text = \"Thes is an example sentnce with speling miskates.\"\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Correct spelling errors\n",
        "corrected_text = blob.correct()\n",
        "print(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NkRManmQhXg",
        "outputId": "8894448a-f3fe-44ac-e099-469ab84988c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The is an example sentence with spelling mistakes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 6- Tokenization"
      ],
      "metadata": {
        "id": "EIs2QHSS5t90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFrsiLVca-DH",
        "outputId": "d7d7a3c1-8761-4cc4-8d4a-9e07fcdde4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thes is an example sentnce with speling miskates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(text))  #tokenize string based on fixed literals such as dot(.)"
      ],
      "metadata": {
        "id": "WxYeai80ly6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024422c5-d35e-42ac-c0c0-e90a49474018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Thes is an example sentnce with speling miskates.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text)) #tokenize each word ,separator is space."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uLZMA04pVIL",
        "outputId": "bfdae670-dd84-4758-cb63-755ca0c7b966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Thes', 'is', 'an', 'example', 'sentnce', 'with', 'speling', 'miskates', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 7-Stemming\n",
        "Stemming is a text normalization technique in natural language processing (NLP) that aims to reduce words to their root or base form, often by removing suffixes. The goal of stemming is to map different inflections or derivations of a word to a common form so that words with the same meaning are treated as equal tokens.\n",
        "\n",
        "It is essential to understand its limitations:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Stemming does not consider the context or meaning of words, which can lead to over-stemming (reducing words to a root form that doesn't make sense) or under-stemming (not reducing words when they should be).\n",
        "\n",
        "1.   Stemming can sometimes produce non-words or words that are not in common use.\n",
        "2.   It may not capture irregular plurals or verb conjugations correctly.\n",
        "3.   Stemming does not consider the context or meaning of words, which can lead to over-stemming (reducing words to a root form that doesn't make sense) or under-stemming (not reducing words when they should be).\n",
        "\n"
      ],
      "metadata": {
        "id": "67LoWGVNQzxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "# Create a stemmer object\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Example words to be stemmed\n",
        "words = [\"running\", \"flies\", \"happily\", \"unhappiness\"]\n",
        "\n",
        "# Apply stemming to the words\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "id": "ED7ZsdwAqiz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15667791-c0bf-4d91-ff50-e0663590a8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'fli', 'happili', 'unhappi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 8- Lemmitization\n",
        "Lemmatization is a text normalization technique in natural language processing (NLP) that reduces words to their base or dictionary form, known as the lemma. Unlike stemming, which crudely removes suffixes from words, lemmatization takes into account the meaning and part of speech of the word. The goal is to transform words to a common and meaningful root form. Lemmatization is more linguistically informed and generally produces valid words."
      ],
      "metadata": {
        "id": "PUtdFi8PRRbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Create a lemmatizer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example words to be lemmatized\n",
        "words = [\"running\", \"flies\", \"happily\", \"unhappiness\"]\n",
        "\n",
        "# Apply lemmatization to the words\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPHYUvHmRQNU",
        "outputId": "57a6be88-261a-495f-989f-b4547cbad23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'fly', 'happily', 'unhappiness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Brief Overview**\n",
        "\n",
        " Here's a step-by-step guide on how to standardize text in NLP:\n",
        "\n",
        "\n",
        "**Lowercasing:**\n",
        "Convert all text to lowercase to ensure uniformity and prevent case sensitivity issues. This is usually done to avoid treating \"Word\" and \"word\" as different tokens.\n",
        "\n",
        "text = text.lower()\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Removing Special Characters:**\n",
        "Remove special characters, punctuation, and symbols from the text, which are not typically essential for many NLP tasks.\n",
        "\n",
        "import re\n",
        "\n",
        "text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Tokenization:**\n",
        "Tokenization is the process of splitting text into individual words or tokens. This step is essential for various NLP tasks.\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Stopword Removal (optional):**\n",
        "Remove common stopwords (e.g., \"the,\" \"is,\" \"and\") if they are not relevant to your analysis.\n",
        "\n",
        "  from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Stemming or Lemmatization (optional):**\n",
        "Reduce words to their base form to handle different word forms (e.g., \"running,\" \"ran\" -> \"run\"). You can choose either stemming or lemmatization, depending on your specific needs.\n",
        "\n",
        "Stemming example using the NLTK library:\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "Lemmatization example using NLTK:\n",
        "\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Handling Contractions (optional):**\n",
        "If you need to expand contractions (e.g., \"I'm\" -> \"I am,\" \"they're\" -> \"they are\"), you can use contraction mapping dictionaries or libraries like pycontractions.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Whitespace Trimming:**\n",
        "Remove extra whitespaces and trim the text.\n",
        "\n",
        "\n",
        "text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "After performing these steps, your text data should be standardized and ready for various NLP tasks like text classification, sentiment analysis, or information retrieval. Remember that the specific preprocessing steps you use can vary depending on the task and the nature of your text data."
      ],
      "metadata": {
        "id": "rZdlHFYaOuN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploring Text data\n"
      ],
      "metadata": {
        "id": "cN6ZmJPU_qOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Basic Statistics:"
      ],
      "metadata": {
        "id": "SapiHFu_DdKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"This is the first document. It contains some words.\",\n",
        "         \"Here is another document with more words.\",\n",
        "         \"The third document is shorter.\"]\n",
        "\n",
        "\n",
        "# Calculate the number of documents\n",
        "num_documents = len(texts)\n",
        "\n",
        "# Calculate the average word count\n",
        "average_word_count = sum(len(text.split()) for text in texts) / num_documents"
      ],
      "metadata": {
        "id": "21tc414GO1ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'No. of documents are ',num_documents)\n",
        "print(f'Average no. of words in each document ',average_word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip7yuiJG_K7d",
        "outputId": "015a2221-450a-4438-88cc-50905d2a1ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of documents are  3\n",
            "Average no. of words in each document  7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part-of-Speech Tagging:"
      ],
      "metadata": {
        "id": "1dco8a80DLRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text documents\n",
        "texts = [\"This is the first document. It contains some words.\",\n",
        "         \"Here is another document with more words.\",\n",
        "         \"The third document is shorter.\",\n",
        "         \"I love second document.It is very nice and well structured \",\n",
        "         \"I'm fed up of these articles.I hate them\"]\n",
        "\n",
        "# Tokenize and perform part-of-speech tagging for each document\n",
        "for text in texts:\n",
        "    words = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waxeabqj_UFZ",
        "outputId": "0124741e-0e9d-43ec-beb8-e86ef534eef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('document', 'NN'), ('.', '.'), ('It', 'PRP'), ('contains', 'VBZ'), ('some', 'DT'), ('words', 'NNS'), ('.', '.')]\n",
            "[('Here', 'RB'), ('is', 'VBZ'), ('another', 'DT'), ('document', 'NN'), ('with', 'IN'), ('more', 'RBR'), ('words', 'NNS'), ('.', '.')]\n",
            "[('The', 'DT'), ('third', 'JJ'), ('document', 'NN'), ('is', 'VBZ'), ('shorter', 'JJR'), ('.', '.')]\n",
            "[('I', 'PRP'), ('love', 'VBP'), ('second', 'JJ'), ('document.It', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('nice', 'JJ'), ('and', 'CC'), ('well', 'RB'), ('structured', 'VBD')]\n",
            "[('I', 'PRP'), (\"'m\", 'VBP'), ('fed', 'VBN'), ('up', 'IN'), ('of', 'IN'), ('these', 'DT'), ('articles.I', 'JJ'), ('hate', 'VBP'), ('them', 'PRP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment Analysis:"
      ],
      "metadata": {
        "id": "3p7ZHdkGDm4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis using TextBlob\n",
        "from textblob import TextBlob\n",
        "for text in texts:\n",
        "  sentiment = TextBlob(text).sentiment\n",
        "  print(sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYClN7Y7_XSE",
        "outputId": "8bd8defa-fd1b-4fd7-fd99-0ac95f4a0028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.25, subjectivity=0.3333333333333333)\n",
            "Sentiment(polarity=0.5, subjectivity=0.5)\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "Sentiment(polarity=0.4266666666666667, subjectivity=0.5333333333333333)\n",
            "Sentiment(polarity=-0.8, subjectivity=0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These objects contain two properties:\n",
        "\n",
        "**Polarity:** It measures the sentiment's positivity or negativity, typically ranging from **-1 (most negative) to 1 (most positive)**. In your examples, the polarities are 0.25, 0.5, and 0.0.\n",
        "\n",
        "**Subjectivity:** It measures the subjectivity of the sentiment, indicating whether the text expresses a **factual or objective statement (closer to 0) or a subjective opinion (closer to 1).**\n",
        "\n",
        "In your examples, the subjectivities are 0.3333 (closer to objective), 0.5 (somewhat subjective), and 0.0 (closer to objective).\n",
        "\n",
        "**Here's how to interpret these examples:**\n",
        "\n",
        "The first example has a polarity of 0.25, indicating a **slightly positive** sentiment, and a subjectivity of approximately 0.33, suggesting that the text contains **some opinion or subjectivity but is relatively objective.**\n",
        "\n",
        "The second example has a polarity of 0.5, indicating a **more positive** sentiment, and a subjectivity of 0.5, indicating a **balanced combination of objectivity and subjectivity**.\n",
        "\n",
        "The third example has a polarity of 0.0, indicating a **neutral sentiment**, and a subjectivity of 0.0, suggesting that the **text is factual and lacks any subjective opinion**.\n",
        "\n",
        "The fourth example has a polarity of 0.4266666666666667, indicates that the text expresses a **more positive sentiment**, and a subjectivity of 0.5333333333333333, suggesting that  **likely includes opinions or subjective language**.\n",
        "\n",
        "The fifth example has a polarity of -0.8, indicates that the text expresses a **very negative sentiment**, and a subjectivity of 0.9, suggesting that  **likely rich in opinions or subjective language**.\n",
        "\n",
        "\n",
        "\n",
        "These sentiment analysis results are often used in applications like opinion mining, social media analysis, and customer feedback analysis to understand the sentiment expressed in text data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6pO4ryvcEcgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Topic Modeling:"
      ],
      "metadata": {
        "id": "sKp0FkYkHN6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic modeling using Latent Dirichlet Allocation (LDA)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
        "lda.fit_transform(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-2crVtfDzC5",
        "outputId": "9f9ca56a-4c2b-4609-e7de-4f1e617a02e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02000495, 0.0202561 , 0.02016091, 0.0200115 , 0.91956654],\n",
              "       [0.02500583, 0.02518463, 0.02512084, 0.02501354, 0.89967516],\n",
              "       [0.03333838, 0.86586453, 0.03358586, 0.03334505, 0.03386619],\n",
              "       [0.01818579, 0.01833237, 0.92690262, 0.01819103, 0.0183882 ],\n",
              "       [0.89997811, 0.02500532, 0.02500357, 0.02501044, 0.02500257]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary Size:"
      ],
      "metadata": {
        "id": "5yKiWP0vIbFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate vocabulary size\n",
        "unique_words = set(\" \".join(texts).split())\n",
        "vocabulary_size = len(unique_words)\n",
        "vocabulary_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOhSdY4lILE-",
        "outputId": "414f5f03-c6d0-4134-8535-c3be3d79ca87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}