{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZvoDoA37XvsIEher3J3Bw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raghavmahajan821/NLP/blob/main/Topic_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#**Topic Modelling**\n",
        "\n",
        "\n",
        "Topic modelling is an unsupervised approach of recognizing or extracting the topics by detecting the patterns like clustering algorithms which divides the data into different parts.\n",
        "One popular topic modelling technique is known as **Latent Dirichlet Allocatio**n (LDA)."
      ],
      "metadata": {
        "id": "GnP2nA2Egd2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Latent Dirichlet Allocation:**\n",
        "\n",
        "In LDA, latent indicates the hidden topics present in the data then Dirichlet is a form of distribution.\n",
        "\n",
        "\n",
        "Dirichlet distribution is different from the normal distribution.\n",
        "\n",
        "The normal distribution represents the data in real numbers format whereas Dirichlet distribution represents the data such that the plotted data sums up to 1."
      ],
      "metadata": {
        "id": "u54B-6Efm0By"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementation of Topic Modelling using LDA:"
      ],
      "metadata": {
        "id": "n9_9mJG5qUdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "0frW1_JCs1R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = \"I am learning NLP, it is very interesting and exciting.it includes machine learning and deep learning\"\n",
        "doc2 = \"My father is a data scientist and he is nlp expert\"\n",
        "doc3 = \"My sister has good exposure into android development\"\n",
        "doc_complete = [doc1, doc2, doc3]\n",
        "doc_complete"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHGKhPl4sYXR",
        "outputId": "7c453d6b-4478-42b2-c784-145614d42483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am learning NLP, it is very interesting and exciting.it includes machine learning and deep learning',\n",
              " 'My father is a data scientist and he is nlp expert',\n",
              " 'My sister has good exposure into android development']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import libraries\n",
        "!pip install gensim\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "# Text preprocessing as discussed in chapter 2\n",
        "stop = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation)\n",
        "lemma = WordNetLemmatizer()\n",
        "def clean(doc):\n",
        "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
        "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        "    return normalized"
      ],
      "metadata": {
        "id": "0mAiuuB5yROR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "818828d3-d9b0-4b85-e72b-20f6faf627b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
        "doc_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyW8n46v0jfp",
        "outputId": "4723057b-1f1e-4828-fd8f-03b22eeb1c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['learning',\n",
              "  'nlp',\n",
              "  'interesting',\n",
              "  'excitingit',\n",
              "  'includes',\n",
              "  'machine',\n",
              "  'learning',\n",
              "  'deep',\n",
              "  'learning'],\n",
              " ['father', 'data', 'scientist', 'nlp', 'expert'],\n",
              " ['sister', 'good', 'exposure', 'android', 'development']]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "# Creating the term dictionary of our corpus, where every unique term is assigned an index.\n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "# Converting a list of documents (corpus) into Document-Term  Matrix using dictionary prepared above.\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "doc_term_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDjK2MDI6r-f",
        "outputId": "25882c2d-f623-414b-b69e-44f4649ca3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (5, 1), (6, 1)],\n",
              " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
              " [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "# Running and Training LDA model on the document term matrix for 3 topics\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word =\n",
        "dictionary, passes=50)\n",
        "# Results\n",
        "print(ldamodel.print_topics())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp7CwZCP63S4",
        "outputId": "db20cc13-41ab-4df8-93a5-4839fe94cca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '0.063*\"nlp\" + 0.063*\"father\" + 0.063*\"data\" + 0.063*\"scientist\" + 0.063*\"expert\" + 0.063*\"good\" + 0.063*\"android\" + 0.063*\"exposure\" + 0.063*\"development\" + 0.063*\"sister\"'), (1, '0.087*\"sister\" + 0.087*\"good\" + 0.087*\"exposure\" + 0.087*\"development\" + 0.087*\"android\" + 0.087*\"father\" + 0.087*\"data\" + 0.087*\"scientist\" + 0.087*\"expert\" + 0.087*\"nlp\"'), (2, '0.232*\"learning\" + 0.093*\"nlp\" + 0.093*\"deep\" + 0.093*\"includes\" + 0.093*\"interesting\" + 0.093*\"machine\" + 0.093*\"excitingit\" + 0.023*\"scientist\" + 0.023*\"data\" + 0.023*\"father\"')]\n"
          ]
        }
      ]
    }
  ]
}